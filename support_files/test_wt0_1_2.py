# compare_predictions.py
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')


class PredictionComparator:
    def __init__(self, model_path: str = "./fine_tuned_rubert_base"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""

        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()

        # üéØ –ß–ï–¢–ö–ò–ï –î–ò–ê–ü–ê–ó–û–ù–´ –ë–ê–õ–õ–û–í –î–õ–Ø –ö–ê–ñ–î–û–ì–û –í–û–ü–†–û–°–ê
        self.question_scores = {
            1: (0, 1),  # –í–æ–ø—Ä–æ—Å 1: –æ—Ç 0 –¥–æ 1 –±–∞–ª–ª–∞
            2: (0, 2),  # –í–æ–ø—Ä–æ—Å 2: –æ—Ç 0 –¥–æ 2 –±–∞–ª–ª–æ–≤
            3: (0, 1),  # –í–æ–ø—Ä–æ—Å 3: –æ—Ç 0 –¥–æ 1 –±–∞–ª–ª–∞
            4: (0, 2)  # –í–æ–ø—Ä–æ—Å 4: –æ—Ç 0 –¥–æ 2 –±–∞–ª–ª–æ–≤
        }

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º!")

    def create_training_text(self, question_text: str, answer_transcription: str, question_num) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ + –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        question_clean = self.preprocess_text(question_text)
        answer_clean = self.preprocess_text(answer_transcription)

        if question_num == 4:
            training_text = answer_clean
        else:
            training_text = f"–í–æ–ø—Ä–æ—Å: {question_clean}\n\n–û—Ç–≤–µ—Ç: {answer_clean}"
        return training_text

    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text) or text is None:
            return ""
        text = str(text).strip()
        text = ' '.join(text.split())
        return text

    def predict_single_text(self, text: str) -> tuple[int, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            inputs = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='pt'
            )

            inputs = {key: value.to(self.device) for key, value in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=1).item()
                confidence = torch.max(predictions).item()

            return predicted_class, confidence

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return 0, 0.0

    def map_class_to_score(self, predicted_class: int, question_number: int) -> int:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –±–∞–ª–ª—ã —Å–æ–≥–ª–∞—Å–Ω–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –≤–æ–ø—Ä–æ—Å–∞"""
        min_score, max_score = self.question_scores[question_number]

        if max_score == 1:  # –í–æ–ø—Ä–æ—Å—ã 1 –∏ 3 (0-1 –±–∞–ª–ª)
            return 0 if predicted_class == 0 else 1
        else:  # –í–æ–ø—Ä–æ—Å—ã 2 –∏ 4 (0-2 –±–∞–ª–ª–∞)
            return min(predicted_class, max_score)

    def predict_test_set(self, test_csv_path: str = "clear_test.csv") -> pd.DataFrame:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞"""
        print("üîÆ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –û–¶–ï–ù–û–ö –î–õ–Ø –¢–ï–°–¢–û–í–û–ì–û –ù–ê–ë–û–†–ê...")
        print("=" * 50)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_df = pd.read_csv(test_csv_path, sep=',')
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_df)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        print(f"\nüîç –°–¢–†–£–ö–¢–£–†–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•:")
        for col in test_df.columns:
            non_null = test_df[col].notna().sum()
            null_percentage = (test_df[col].isna().sum() / len(test_df)) * 100
            print(f"   {col}: {non_null} –Ω–µ–ø—É—Å—Ç—ã—Ö ({null_percentage:.1f}% –ø—É—Å—Ç—ã—Ö)")

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        predictions = []

        print(f"\nüéØ –ù–ê–ß–ê–õ–û –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô...")
        print("-" * 40)

        for idx, row in test_df.iterrows():
            question_num = row['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']
            question_text = row['–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞']
            answer_text = row['–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']
            exam_id = row['Id —ç–∫–∑–∞–º–µ–Ω–∞']
            question_id = row['Id –≤–æ–ø—Ä–æ—Å–∞']

            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            training_text = self.create_training_text(question_text, answer_text, question_num)

            if not training_text or training_text == "–í–æ–ø—Ä–æ—Å: \n\n–û—Ç–≤–µ—Ç: ":
                print(f"‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç–∫–∑–∞–º–µ–Ω–∞ {exam_id}, –≤–æ–ø—Ä–æ—Å {question_num}")
                predictions.append({
                    'Id —ç–∫–∑–∞–º–µ–Ω–∞': exam_id,
                    'Id –≤–æ–ø—Ä–æ—Å–∞': question_id,
                    '‚Ññ –≤–æ–ø—Ä–æ—Å–∞': question_num,
                    'predicted_class': 0,
                    'predicted_score': 0,
                    'confidence': 0.0,
                    'error': 'empty_data'
                })
                continue

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predicted_class, confidence = self.predict_single_text(training_text)
            predicted_score = self.map_class_to_score(predicted_class, question_num)

            predictions.append({
                'Id —ç–∫–∑–∞–º–µ–Ω–∞': exam_id,
                'Id –≤–æ–ø—Ä–æ—Å–∞': question_id,
                '‚Ññ –≤–æ–ø—Ä–æ—Å–∞': question_num,
                'predicted_class': predicted_class,
                'predicted_score': predicted_score,
                'confidence': confidence,
                'max_score': self.question_scores[question_num][1]
            })

            if (idx + 1) % 100 == 0:
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1}/{len(test_df)} –∑–∞–ø–∏—Å–µ–π...")

        predictions_df = pd.DataFrame(predictions)
        print(f"üìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã –¥–ª—è {len(predictions_df)} –∑–∞–ø–∏—Å–µ–π")

        return predictions_df

    def load_true_marks(self, marks_csv_path: str = "clear_test_with_marks.csv") -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫"""
        print(f"\nüìñ –ó–ê–ì–†–£–ó–ö–ê –ü–†–ê–í–ò–õ–¨–ù–´–• –û–¶–ï–ù–û–ö...")
        marks_df = pd.read_csv(marks_csv_path, sep=',')
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(marks_df)} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ü–µ–Ω–∫–∞–º–∏")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        if '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' not in marks_df.columns:
            print("‚ùå –í —Ñ–∞–π–ª–µ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'")
            return pd.DataFrame()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ü–µ–Ω–∫–∏
        non_null_marks = marks_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].notna().sum()
        print(f"üìù –ù–∞–π–¥–µ–Ω–æ {non_null_marks} –Ω–µ–ø—É—Å—Ç—ã—Ö –æ—Ü–µ–Ω–æ–∫")

        return marks_df[['Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']]

    def compare_predictions(self, predictions_df: pd.DataFrame, true_marks_df: pd.DataFrame):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏"""
        print(f"\nüîç –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –û–¶–ï–ù–ö–ê–ú–ò...")
        print("=" * 60)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        comparison_df = pd.merge(
            predictions_df,
            true_marks_df,
            on=['Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞'],
            how='inner'
        )

        print(f"üìä –£—Å–ø–µ—à–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {len(comparison_df)} –∑–∞–ø–∏—Å–µ–π")

        if len(comparison_df) == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã.")
            return

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ —á–∏—Å–ª–æ–≤—ã–µ
        comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'] = pd.to_numeric(comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'], errors='coerce')
        comparison_df = comparison_df.dropna(subset=['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'])

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        comparison_df['match'] = comparison_df['predicted_score'] == comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']

        # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        overall_accuracy = comparison_df['match'].mean() * 100
        print(f"\nüéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {overall_accuracy:.2f}%")

        # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
        print(f"\nüìä –¢–û–ß–ù–û–°–¢–¨ –ü–û –í–û–ü–†–û–°–ê–ú:")
        for question_num in range(1, 5):
            question_data = comparison_df[comparison_df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == question_num]
            if len(question_data) > 0:
                accuracy = question_data['match'].mean() * 100
                max_score = self.question_scores[question_num][1]
                print(f"   –í–æ–ø—Ä–æ—Å {question_num} (0-{max_score}): {accuracy:.2f}% ({len(question_data)} –∑–∞–ø–∏—Å–µ–π)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        high_confidence = comparison_df[comparison_df['confidence'] > 0.7]
        if len(high_confidence) > 0:
            high_conf_accuracy = high_confidence['match'].mean() * 100
            print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ >0.7: {high_conf_accuracy:.2f}% ({len(high_confidence)} –∑–∞–ø–∏—Å–µ–π)")

        return comparison_df

    def generate_detailed_report(self, comparison_df: pd.DataFrame):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏"""
        print(f"\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –û –°–†–ê–í–ù–ï–ù–ò–ò")
        print("=" * 60)

        # –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º', fontsize=16, fontweight='bold')

        for i, question_num in enumerate(range(1, 5)):
            ax = axes[(i) // 2, (i) % 2]
            question_data = comparison_df[comparison_df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == question_num]

            if len(question_data) > 0:
                y_true = question_data['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']
                y_pred = question_data['predicted_score']

                # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
                cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true) | set(y_pred)))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                            xticklabels=sorted(set(y_true) | set(y_pred)),
                            yticklabels=sorted(set(y_true) | set(y_pred)))

                ax.set_title(f'–í–æ–ø—Ä–æ—Å {question_num} (0-{self.question_scores[question_num][1]})')
                ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏')
                ax.set_ylabel('–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏')

        plt.tight_layout()
        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
        print(f"üíæ –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'confusion_matrices.png'")

        # Classification report –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        print(f"\nüìã –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–û–ù–ù–´–ï –û–¢–ß–ï–¢–´:")
        for question_num in range(1, 5):
            question_data = comparison_df[comparison_df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == question_num]
            if len(question_data) > 0:
                y_true = question_data['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']
                y_pred = question_data['predicted_score']

                print(f"\n‚ùì –í–æ–ø—Ä–æ—Å {question_num}:")
                print(classification_report(y_true, y_pred, digits=3))

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫
        plt.figure(figsize=(12, 8))

        plt.subplot(2, 2, 1)
        comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].value_counts().sort_index().plot(kind='bar', color='lightblue')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫')
        plt.xlabel('–û—Ü–µ–Ω–∫–∞')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')

        plt.subplot(2, 2, 2)
        comparison_df['predicted_score'].value_counts().sort_index().plot(kind='bar', color='lightgreen')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫')
        plt.xlabel('–û—Ü–µ–Ω–∫–∞')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')

        plt.subplot(2, 2, 3)
        comparison_df['confidence'].hist(bins=30, color='lightcoral', alpha=0.7)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏')
        plt.xlabel('–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')

        plt.subplot(2, 2, 4)
        match_by_confidence = comparison_df.groupby(pd.cut(comparison_df['confidence'], bins=10))['match'].mean()
        match_by_confidence.plot(kind='bar', color='gold')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
        plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xticks(rotation=45)

        plt.tight_layout()
        plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')
        print(f"üíæ –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'prediction_analysis.png'")

        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {comparison_df['confidence'].mean():.3f}")
        print(
            f"   –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {abs(comparison_df['predicted_score'] - comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']).mean():.3f}")

        # F1-score –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
        print(f"\nüéØ F1-–ú–ï–¢–†–ò–ö–ò –ü–û –í–û–ü–†–û–°–ê–ú:")
        for question_num in range(1, 5):
            question_data = comparison_df[comparison_df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == question_num]
            if len(question_data) > 0:
                y_true = question_data['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']
                y_pred = question_data['predicted_score']
                f1 = f1_score(y_true, y_pred, average='weighted')
                print(f"   –í–æ–ø—Ä–æ—Å {question_num}: F1 = {f1:.3f}")

    def save_comparison_results(self, comparison_df: pd.DataFrame, predictions_df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")

        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        detailed_results = comparison_df[[
            'Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞',
            '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞', 'predicted_score', 'predicted_class',
            'confidence', 'match', 'max_score'
        ]]

        detailed_results.to_csv('detailed_comparison_results.csv', index=False, sep=';')
        print("   –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: 'detailed_comparison_results.csv'")

        # –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions_df.to_csv('all_predictions.csv', index=False, sep=';')
        print("   –í—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 'all_predictions.csv'")

        # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        summary = {
            'total_records': len(comparison_df),
            'overall_accuracy': comparison_df['match'].mean() * 100,
            'mean_confidence': comparison_df['confidence'].mean(),
            'mean_absolute_error': abs(comparison_df['predicted_score'] - comparison_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞']).mean()
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º
        for question_num in range(1, 5):
            question_data = comparison_df[comparison_df['‚Ññ –≤–æ–ø—Ä–æ—Å–∞'] == question_num]
            if len(question_data) > 0:
                accuracy = question_data['match'].mean() * 100
                summary[f'accuracy_question_{question_num}'] = accuracy

        summary_df = pd.DataFrame([summary])
        summary_df.to_csv('comparison_summary.csv', index=False, sep=';')
        print("   –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: 'comparison_summary.csv'")

        print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    print("üöÄ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –î–õ–Ø –°–†–ê–í–ù–ï–ù–ò–Ø...")
    comparator = PredictionComparator("./fine_tuned_rubert_base")

    # –®–∞–≥ 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è clear_test.csv
    predictions_df = comparator.predict_test_set("clear_test.csv")

    if predictions_df.empty:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        return

    # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    true_marks_df = comparator.load_true_marks("clear_test_with_marks.csv")

    if true_marks_df.empty:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏")
        return

    # –®–∞–≥ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
    comparison_df = comparator.compare_predictions(predictions_df, true_marks_df)

    if comparison_df is None or comparison_df.empty:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        return

    # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    comparator.generate_detailed_report(comparison_df)

    # –®–∞–≥ 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    comparator.save_comparison_results(comparison_df, predictions_df)

    print(f"\nüéâ –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(comparison_df)} –∑–∞–ø–∏—Å–µ–π")


if __name__ == "__main__":
    main()
