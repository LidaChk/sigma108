import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import warnings
import os

warnings.filterwarnings('ignore')

# üîß –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø M1 PRO
DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")


# üìÅ –ö–ª–∞—Å—Å –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
class ExamDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


# üìä –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1_macro = f1_score(labels, predictions, average='macro')
    f1_weighted = f1_score(labels, predictions, average='weighted')
    f1_per_class = f1_score(labels, predictions, average=None)

    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'f1_class_0': f1_per_class[0],
        'f1_class_1': f1_per_class[1],
        'f1_class_2': f1_per_class[2],
    }


# üéØ –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å Weighted Loss
class WeightedTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)
        if hasattr(self.model, 'device'):
            self.class_weights = self.class_weights.to(self.model.device)
        else:
            self.class_weights = self.class_weights.to(DEVICE)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)
        loss = loss_fct(logits, labels)

        return (loss, outputs) if return_outputs else loss


def train_rubert_base():
    """–û–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ RuBERT-base"""

    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø RuBERT-base")
    print("=" * 50)

    # üîß –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø RuBERT-base
    MODEL_NAME = "ai-forever/ruBert-base"  # ‚úÖ –ë–æ–ª–µ–µ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å
    BATCH_SIZE = 4  # ‚úÖ –£–º–µ–Ω—å—à–∞–µ–º batch size –∏–∑-–∑–∞ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
    MAX_LENGTH = 256  # ‚úÖ –£–º–µ–Ω—å—à–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    LEARNING_RATE = 1e-5  # ‚úÖ –ú–µ–Ω—å—à–µ learning rate –¥–ª—è –±–æ–ª—å—à–µ–π –º–æ–¥–µ–ª–∏
    EPOCHS = 8  # ‚úÖ –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö (–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ)
    WARMUP_RATIO = 0.1

    # üìÅ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_df = pd.read_csv('train_text_dataset.csv', sep=';')
    val_df = pd.read_csv('val_text_dataset.csv', sep=';')

    print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    train_counts = train_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].value_counts().sort_index()
    print("\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    for score, count in train_counts.items():
        percentage = (count / len(train_df)) * 100
        print(f"  –û—Ü–µ–Ω–∫–∞ {score}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.1f}%)")

    # üéØ –í–ï–°–ê –ö–õ–ê–°–°–û–í (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è rubert-base)
    class_weights = np.array([3.5, 0.5, 0.9])  # ‚úÖ –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞ 0
    print(f"\n‚öñÔ∏è –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")

    # ü§ñ –ó–ê–ì–†–£–ó–ö–ê RuBERT-base
    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ RuBERT-base...")
    print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–∞—è (~700–ú–ë), –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è...")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=3,
        id2label={0: "–æ—Ü–µ–Ω–∫–∞_0", 1: "–æ—Ü–µ–Ω–∫–∞_1", 2: "–æ—Ü–µ–Ω–∫–∞_2"},
        label2id={"–æ—Ü–µ–Ω–∫–∞_0": 0, "–æ—Ü–µ–Ω–∫–∞_1": 1, "–æ—Ü–µ–Ω–∫–∞_2": 2},
        ignore_mismatched_sizes=True
    )

    # üìö –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í
    print("üìö –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    train_dataset = ExamDataset(
        texts=train_df['—Ç–µ–∫—Å—Ç_–¥–ª—è_–æ–±—É—á–µ–Ω–∏—è'].values,
        labels=train_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].values,
        tokenizer=tokenizer,
        max_length=MAX_LENGTH
    )

    val_dataset = ExamDataset(
        texts=val_df['—Ç–µ–∫—Å—Ç_–¥–ª—è_–æ–±—É—á–µ–Ω–∏—è'].values,
        labels=val_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].values,
        tokenizer=tokenizer,
        max_length=MAX_LENGTH
    )

    # ‚öôÔ∏è –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø RuBERT-base
    training_args = TrainingArguments(
        output_dir='./rubert_base_results',
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        warmup_ratio=WARMUP_RATIO,
        weight_decay=0.01,
        logging_dir='./rubert_base_logs',
        logging_steps=100,
        eval_steps=100,
        save_steps=200,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        greater_is_better=True,
        report_to=None,
        fp16=False,  # ‚ùå –õ—É—á—à–µ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ M1
        dataloader_pin_memory=False,
        save_total_limit=3,
        no_cuda=True if DEVICE == "mps" else False,
        remove_unused_columns=False,
        gradient_accumulation_steps=2,  # ‚úÖ –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ batch size = 8
        dataloader_num_workers=0,
    )

    # üéØ –°–û–ó–î–ê–ù–ò–ï TRAINER
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ trainer –¥–ª—è RuBERT-base...")
    trainer = WeightedTrainer(
        class_weights=class_weights,
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    # üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RuBERT-base...")
    print(f"üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø RuBERT-base:")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    print(f"   –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * 2} —Å gradient accumulation)")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Epochs: {EPOCHS}")
    print(f"   –ú–∞–∫—Å. –¥–ª–∏–Ω–∞: {MAX_LENGTH}")
    print(f"   –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–∞ 0: {class_weights[0]}")

    if DEVICE == "mps":
        model = model.to(DEVICE)
        print(f"   –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ MPS")

    print(f"\nüéØ –û–ñ–ò–î–ê–ï–ú–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:")
    print(f"   Accuracy: 75.1% ‚Üí 80-85%")
    print(f"   F1-macro: 69.8% ‚Üí 75-80%")
    print(f"   Class 0 F1: 48.8% ‚Üí 60-70%")

    print(f"\n‚è≥ –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 2-4 —á–∞—Å–∞")
    print(f"üíæ –ü–∞–º—è—Ç—å: ~4-6GB RAM")

    # üèÅ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
    train_result = trainer.train()

    # üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ RuBERT-base...")
    trainer.save_model("./fine_tuned_rubert_base")
    tokenizer.save_pretrained("./fine_tuned_rubert_base")

    # üìä –û–¶–ï–ù–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    print("üìä –û—Ü–µ–Ω–∫–∞ RuBERT-base...")
    eval_results = trainer.evaluate()

    print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ RuBERT-base:")
    print("=" * 50)
    for key, value in eval_results.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")

    # üìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢
    print("\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    predictions = trainer.predict(val_dataset)
    pred_labels = np.argmax(predictions.predictions, axis=1)
    true_labels = val_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].values

    print(classification_report(true_labels, pred_labels,
                                target_names=['–û—Ü–µ–Ω–∫–∞ 0', '–û—Ü–µ–Ω–∫–∞ 1', '–û—Ü–µ–Ω–∫–∞ 2'],
                                digits=4))

    # üìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –°–†–ê–í–ù–ï–ù–ò–Ø
    plot_comparison_with_previous(true_labels, pred_labels, eval_results)

    return trainer, eval_results


def plot_comparison_with_previous(true_labels, pred_labels, eval_results):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RuBERT-base —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª—å—é"""
    try:
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ (rubert-tiny2)
        previous_results = {
            'accuracy': 0.7510,
            'f1_macro': 0.6985,
            'f1_class_0': 0.4880,
            'f1_class_1': 0.7640,
            'f1_class_2': 0.8435
        }

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: RuBERT-base vs RuBERT-tiny2', fontsize=16, fontweight='bold')

        # 1. Confusion Matrix –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        ax1 = axes[0, 0]
        cm = confusion_matrix(true_labels, pred_labels)
        im = ax1.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        ax1.set_title('RuBERT-base: Confusion Matrix', fontweight='bold')
        plt.colorbar(im, ax=ax1)
        tick_marks = np.arange(3)
        ax1.set_xticks(tick_marks)
        ax1.set_yticks(tick_marks)
        ax1.set_xticklabels(['0', '1', '2'])
        ax1.set_yticklabels(['0', '1', '2'])
        ax1.set_xlabel('Predicted')
        ax1.set_ylabel('True')

        thresh = cm.max() / 2.
        for i, j in np.ndindex(cm.shape):
            ax1.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black",
                     fontweight='bold')

        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        ax2 = axes[0, 1]
        metrics = ['Accuracy', 'F1 Macro']
        previous_values = [previous_results['accuracy'], previous_results['f1_macro']]
        current_values = [eval_results['eval_accuracy'], eval_results['eval_f1_macro']]

        x = np.arange(len(metrics))
        width = 0.35

        bars1 = ax2.bar(x - width / 2, previous_values, width, label='RuBERT-tiny2',
                        color='lightblue', edgecolor='black', alpha=0.8)
        bars2 = ax2.bar(x + width / 2, current_values, width, label='RuBERT-base',
                        color='lightgreen', edgecolor='black', alpha=0.8)

        ax2.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫', fontweight='bold')
        ax2.set_ylabel('Score')
        ax2.set_xticks(x)
        ax2.set_xticklabels(metrics)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1)

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è
        for i, (prev, curr) in enumerate(zip(previous_values, current_values)):
            improvement = curr - prev
            color = 'green' if improvement > 0 else 'red'
            ax2.text(i, max(prev, curr) + 0.02, f'+{improvement:.3f}',
                     ha='center', va='bottom', fontweight='bold', color=color)

        # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1 –ø–æ –∫–ª–∞—Å—Å–∞–º
        ax3 = axes[1, 0]
        classes = ['–û—Ü–µ–Ω–∫–∞ 0', '–û—Ü–µ–Ω–∫–∞ 1', '–û—Ü–µ–Ω–∫–∞ 2']
        previous_f1 = [previous_results['f1_class_0'], previous_results['f1_class_1'], previous_results['f1_class_2']]
        current_f1 = [eval_results['eval_f1_class_0'], eval_results['eval_f1_class_1'], eval_results['eval_f1_class_2']]

        x = np.arange(len(classes))

        bars1 = ax3.bar(x - width / 2, previous_f1, width, label='RuBERT-tiny2',
                        color='lightblue', edgecolor='black', alpha=0.8)
        bars2 = ax3.bar(x + width / 2, current_f1, width, label='RuBERT-base',
                        color='lightgreen', edgecolor='black', alpha=0.8)

        ax3.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1 –ø–æ –∫–ª–∞—Å—Å–∞–º', fontweight='bold')
        ax3.set_ylabel('F1 Score')
        ax3.set_xticks(x)
        ax3.set_xticklabels(classes)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)

        # –î–æ–±–∞–≤–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è
        for i, (prev, curr) in enumerate(zip(previous_f1, current_f1)):
            improvement = curr - prev
            color = 'green' if improvement > 0 else 'red'
            ax3.text(i, max(prev, curr) + 0.02, f'+{improvement:.3f}',
                     ha='center', va='bottom', fontweight='bold', color=color)

        # 4. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        ax4 = axes[1, 1]
        ax4.axis('off')

        total_improvement = (eval_results['eval_accuracy'] - previous_results['accuracy'] +
                             eval_results['eval_f1_macro'] - previous_results['f1_macro']) / 2

        model_info = [
            f"üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:",
            f"",
            f"RuBERT-tiny2:",
            f"  ‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~30M",
            f"  ‚Ä¢ Accuracy: {previous_results['accuracy']:.3f}",
            f"  ‚Ä¢ F1 Macro: {previous_results['f1_macro']:.3f}",
            f"  ‚Ä¢ Class 0 F1: {previous_results['f1_class_0']:.3f}",
            f"",
            f"RuBERT-base:",
            f"  ‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~178M",
            f"  ‚Ä¢ Accuracy: {eval_results['eval_accuracy']:.3f}",
            f"  ‚Ä¢ F1 Macro: {eval_results['eval_f1_macro']:.3f}",
            f"  ‚Ä¢ Class 0 F1: {eval_results['eval_f1_class_0']:.3f}",
            f"",
            f"üìà –û–ë–©–ï–ï –£–õ–£–ß–®–ï–ù–ò–ï: {total_improvement:.3f}",
        ]

        for i, text in enumerate(model_info):
            if "–°–†–ê–í–ù–ï–ù–ò–ï" in text or "–û–ë–©–ï–ï –£–õ–£–ß–®–ï–ù–ò–ï" in text:
                ax4.text(0.1, 0.95 - i * 0.05, text, transform=ax4.transAxes,
                         fontsize=11, verticalalignment='top', fontweight='bold',
                         color='darkblue')
            elif "RuBERT-base" in text:
                ax4.text(0.1, 0.95 - i * 0.05, text, transform=ax4.transAxes,
                         fontsize=10, verticalalignment='top', fontweight='bold',
                         color='darkgreen')
            else:
                ax4.text(0.1, 0.95 - i * 0.05, text, transform=ax4.transAxes,
                         fontsize=10, verticalalignment='top', fontweight='normal')

        plt.tight_layout()
        plt.savefig('rubert_base_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')
        print("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'rubert_base_comparison.png'")
        plt.close()

    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")


def check_memory_usage():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    import psutil
    memory = psutil.virtual_memory()
    print(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory.percent}%")
    print(f"   –î–æ—Å—Ç—É–ø–Ω–æ: {memory.available / 1024 / 1024 / 1024:.1f} GB")
    print(f"   –í—Å–µ–≥–æ: {memory.total / 1024 / 1024 / 1024:.1f} GB")


if __name__ == "__main__":
    try:
        # üîç –ü–†–û–í–ï–†–ö–ê –ü–ê–ú–Ø–¢–ò
        check_memory_usage()

        # üöÄ –û–ë–£–ß–ï–ù–ò–ï RuBERT-base
        trainer, results = train_rubert_base()

        # üìà –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
        previous_accuracy = 0.7510
        previous_f1_macro = 0.6985
        previous_class0_f1 = 0.4880

        accuracy_improvement = results['eval_accuracy'] - previous_accuracy
        f1_improvement = results['eval_f1_macro'] - previous_f1_macro
        class0_improvement = results['eval_f1_class_0'] - previous_class0_f1

        print("\n" + "=" * 60)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï RuBERT-base –ó–ê–í–ï–†–®–ï–ù–û!")
        print("=" * 60)

        print(f"\nüíæ –ú–û–î–ï–õ–¨ –°–û–•–†–ê–ù–ï–ù–ê:")
        print(f"   ./fine_tuned_rubert_base")

        print(f"\nüìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø:")
        print(f"   rubert_base_comparison.png")

        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø:")
        print(f"   Accuracy: {previous_accuracy:.3f} ‚Üí {results['eval_accuracy']:.3f} ({accuracy_improvement:+.3f})")
        print(f"   F1 Macro: {previous_f1_macro:.3f} ‚Üí {results['eval_f1_macro']:.3f} ({f1_improvement:+.3f})")
        print(f"   Class 0 F1: {previous_class0_f1:.3f} ‚Üí {results['eval_f1_class_0']:.3f} ({class0_improvement:+.3f})")

        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        if accuracy_improvement > 0.03:
            print(f"   üéâ –û—Ç–ª–∏—á–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ! RuBERT-base –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ.")
        elif accuracy_improvement > 0.01:
            print(f"   ‚úÖ –•–æ—Ä–æ—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ. RuBERT-base –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")
        else:
            print(f"   ‚ö†Ô∏è –£–ª—É—á—à–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥—Ä—É–≥–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è.")

        if class0_improvement > 0.05:
            print(f"   üéâ –ö–ª–∞—Å—Å 0 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª—Å—è! +{class0_improvement:.3f}")
        elif class0_improvement > 0.02:
            print(f"   ‚úÖ –ö–ª–∞—Å—Å 0 —É–ª—É—á—à–∏–ª—Å—è. +{class0_improvement:.3f}")
        else:
            print(f"   ‚ö†Ô∏è –ö–ª–∞—Å—Å 0 –≤—Å–µ –µ—â–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è.")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ RuBERT-base: {e}")
        import traceback

        traceback.print_exc()

        # –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø–∞–º—è—Ç—å—é
        if "CUDA out of memory" in str(e) or "memory" in str(e).lower():
            print(f"\nüí° –°–û–í–ï–¢: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å BATCH_SIZE –¥–æ 2 –∏–ª–∏ MAX_LENGTH –¥–æ 128")

