import os
import re
import pandas as pd
from bs4 import BeautifulSoup
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime

# ---------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# ---------------------------
BASE_MODEL = "Qwen/Qwen3-0.6B"
ADAPTER_PATH = "./qwen-lora-m1/checkpoint-1839"
INPUT_CSV = "clear_test_with_marks.csv"  # –∑–¥–µ—Å—å –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
OUTPUT_CSV = "result_with_stats.csv"
LOG_FILE = "inference_log.txt"

MAX_INPUT_LENGTH = 512
MAX_NEW_TOKENS = 8
BATCH_SIZE = 1  # –¥–ª—è M1 Pro –±–µ–∑–æ–ø–∞—Å–Ω–æ

# ---------------------------
# –£—Ç–∏–ª–∏—Ç—ã
# ---------------------------
def log(msg: str):
    """–ü–∏—à–µ—Ç –ª–æ–≥ –∏ –Ω–∞ —ç–∫—Ä–∞–Ω, –∏ –≤ —Ñ–∞–π–ª"""
    print(msg)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}\n")

def clean_html(text):
    if pd.isna(text):
        return ""
    return BeautifulSoup(str(text), "html.parser").get_text(separator=" ").strip()

def build_prompt(row):
    question_number = row.get("‚Ññ –≤–æ–ø—Ä–æ—Å–∞", "")
    question_text = clean_html(row.get("–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞", ""))
    answer_text = str(row.get("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞", ""))
    return f"–í–æ–ø—Ä–æ—Å ‚Ññ{question_number}: {question_text}\n–û—Ç–≤–µ—Ç: {answer_text}\n–ü—Ä–µ–¥—Å–∫–∞–∂–∏ –æ—Ü–µ–Ω–∫—É —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞ (—á–∏—Å–ª–æ):"

def extract_number_from_text(text):
    m = re.search(r"\b([0-2])\b", text)
    if m:
        return int(m.group(1))
    m2 = re.search(r"(-?\d+)", text)
    if m2:
        val = int(m2.group(1))
        return min(max(val, 0), 2)
    return None

# ---------------------------
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
# ---------------------------
log("–û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ...")
device = "mps" if torch.backends.mps.is_available() else "cpu"
log(f"Device: {device}")

log("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

log("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map={"": device},
    dtype=torch.float16 if device == "mps" else torch.float32,
    low_cpu_mem_usage=True,
)

try:
    log(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä –∏–∑ {ADAPTER_PATH} ...")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, device_map={"": device})
    log("‚úÖ –ê–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—ë–Ω.")
except Exception as e:
    log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–¥–∞–ø—Ç–µ—Ä: {e}")
    model = base_model

model.eval()
log(f"–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞. dtype: {next(model.parameters()).dtype}")

# ---------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ---------------------------
log(f"–ß–∏—Ç–∞–µ–º CSV: {INPUT_CSV}")
df = pd.read_csv(INPUT_CSV, sep=";", encoding="utf-8")
df_subset = df.iloc[:len(df)//30]
prompts = df_subset.apply(build_prompt, axis=1).tolist()
preds = []

# ---------------------------
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
# ---------------------------
log(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å ({len(prompts)} –∑–∞–ø–∏—Å–µ–π)...")
for i in range(0, len(prompts), BATCH_SIZE):
    batch_prompts = prompts[i:i + BATCH_SIZE]
    enc = tokenizer(
        batch_prompts,
        padding=True,
        truncation=True,
        max_length=MAX_INPUT_LENGTH,
        return_tensors="pt"
    )

    for k, v in enc.items():
        enc[k] = v.to(device)

    with torch.no_grad():
        outputs = model.generate(
            **enc,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )

    for j, out in enumerate(outputs):
        text = tokenizer.decode(out, skip_special_tokens=True)
        prompt = batch_prompts[j]
        gen_part = text.split("–ü—Ä–µ–¥—Å–∫–∞–∂–∏ –æ—Ü–µ–Ω–∫—É —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞ (—á–∏—Å–ª–æ):")[-1].strip()
        num = extract_number_from_text(gen_part)

        preds.append({"generated_text": gen_part, "predicted_score": num})
        log(f"[{i + j + 1}/{len(prompts)}] ‚Üí {gen_part} ‚Üí {num}")

# ---------------------------
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
# ---------------------------
result = pd.concat([df.reset_index(drop=True), pd.DataFrame(preds)], axis=1)

if "–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞" in result.columns:
    result["correct"] = result["predicted_score"] == result["–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞"]
    total_correct = result["correct"].sum()
    total = len(result)
    acc = total_correct / total * 100

    log(f"\nüìä –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {acc:.2f}% ({total_correct}/{total})")

    per_question = (
        result.groupby("‚Ññ –≤–æ–ø—Ä–æ—Å–∞")["correct"]
        .agg(["count", "sum"])
        .reset_index()
    )
    per_question["accuracy_%"] = per_question["sum"] / per_question["count"] * 100
    log("\nüìà –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º:")
    for _, row in per_question.iterrows():
        log(f"  –í–æ–ø—Ä–æ—Å {row['‚Ññ –≤–æ–ø—Ä–æ—Å–∞']}: {row['accuracy_%']:.1f}% ({int(row['sum'])}/{int(row['count'])})")

else:
    log("‚ö†Ô∏è –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞' ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞.")

# ---------------------------
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# ---------------------------
result.to_csv(OUTPUT_CSV, index=False)
log(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_CSV}")
