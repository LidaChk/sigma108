import pandas as pd
import numpy as np
import torch
import re
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from config import CSV_COLUMN_CONFIG, QUESTION_NUMBER_COLUMN_NAME, SCORE_COLUMN_NAME, CSV_ENCODING
from logger.log_config import logger


def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML-—Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if pd.isna(text):
        return ""

    text = str(text)
    # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', text)
    # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É, –ª–∞—Ç–∏–Ω–∏—Ü—É –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;:()\-]', '', text)
    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

class ModelPredictor:
        def __init__(self, model_path: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫"""
        try:
            # üîß –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–£–¢–ò –ö –ú–û–î–ï–õ–ò
            if model_path is None:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (—Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª: backend/services/file_processor.py)
                current_file = Path(__file__)
                base_dir = current_file.parent.parent  # backend/

                # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: backend/fine_tuned_rubert_base
                model_path = str(base_dir / "fine_tuned_rubert_base")

                logger.info(f"üîç –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –ø–æ –ø—É—Ç–∏: {model_path}")

            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {model_path}")

            # üîß –ü–†–û–í–ï–†–ö–ê –°–£–©–ï–°–¢–í–û–í–ê–ù–ò–Ø –ú–û–î–ï–õ–ò
            model_dir = Path(model_path)
            if not model_dir.exists():
                error_msg = f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}"
                logger.error(error_msg)

                # –ü–æ–∫–∞–∂–µ–º —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                if model_dir.parent.exists():
                    available = [f.name for f in model_dir.parent.iterdir() if f.is_dir()]
                    logger.error(f"üìÇ –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ {model_dir.parent}: {available}")

                raise FileNotFoundError(error_msg)

            # üîß –ó–ê–ì–†–£–ó–ö–ê –° –õ–û–ö–ê–õ–¨–ù–´–ú–ò –§–ê–ô–õ–ê–ú–ò
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_dir),
                local_files_only=True
            )

            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
            self.model = AutoModelForSequenceClassification.from_pretrained(
                str(model_dir),
                local_files_only=True
            )

            self.model.to(self.device)
            self.model.eval()

            # –î–∏–∞–ø–∞–∑–æ–Ω—ã –±–∞–ª–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            self.question_scores = {
                1: (0, 1),  # –í–æ–ø—Ä–æ—Å 1: –æ—Ç 0 –¥–æ 1 –±–∞–ª–ª–∞
                2: (0, 2),  # –í–æ–ø—Ä–æ—Å 2: –æ—Ç 0 –¥–æ 2 –±–∞–ª–ª–æ–≤
                3: (0, 1),  # –í–æ–ø—Ä–æ—Å 3: –æ—Ç 0 –¥–æ 1 –±–∞–ª–ª–∞
                4: (0, 2)  # –í–æ–ø—Ä–æ—Å 4: –æ—Ç 0 –¥–æ 2 –±–∞–ª–ª–æ–≤
            }

            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º!")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def create_training_text(self, question_text: str, answer_transcription: str, question_num) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ + –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        question_clean = self.preprocess_text(question_text)
        answer_clean = self.preprocess_text(answer_transcription)

        if question_num == 4:
            training_text = answer_clean
        else:
            training_text = f"–í–æ–ø—Ä–æ—Å: {question_clean}\n\n–û—Ç–≤–µ—Ç: {answer_clean}"
        return training_text

    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π –æ—Ç HTML-—Ç–µ–≥–æ–≤"""
        if pd.isna(text) or text is None:
            return ""

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é clean_text –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        text = clean_text(text)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
        text = str(text).strip()
        text = ' '.join(text.split())
        return text

    def predict_single_text(self, text: str) -> tuple[int, str]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –≤—Å–µ—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        try:
            inputs = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='pt'
            )

            inputs = {key: value.to(self.device) for key, value in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=1).item()

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
                probs = predictions[0].cpu().numpy()
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É —Ç–∏–ø–∞ "0=0.4771, 1=0.5090, 2=0.0138"
                confidence_str = ", ".join([f"{i}={probs[i]:.4f}" for i in range(len(probs))])

            return predicted_class, confidence_str

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return 0, "0=0.0000, 1=0.0000, 2=0.0000"

    def map_class_to_score(self, predicted_class: int, question_number: int) -> int:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –±–∞–ª–ª—ã"""
        if question_number not in self.question_scores:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –Ω–æ–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞: {question_number}, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω 0")
            return 0

        min_score, max_score = self.question_scores[question_number]

        if max_score == 1:  # –í–æ–ø—Ä–æ—Å—ã 1 –∏ 3 (0-1 –±–∞–ª–ª)
            return 0 if predicted_class == 0 else 1
        else:  # –í–æ–ø—Ä–æ—Å—ã 2 –∏ 4 (0-2 –±–∞–ª–ª–∞)
            return min(predicted_class, max_score)


def process_csv_with_model(input_file_path: Path, output_file_path: Path, model_path: str = None):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV-—Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏.
    –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    """
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {input_file_path}")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        try:
            df = pd.read_csv(input_file_path, sep=';', encoding=CSV_ENCODING)
            logger.info("–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';'")
        except pd.errors.ParserError:
            df = pd.read_csv(input_file_path, sep=',', encoding=CSV_ENCODING)
            logger.info("–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ','")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = set(CSV_COLUMN_CONFIG["INPUT_COLUMNS"])
        if not all(col in df.columns for col in required_columns):
            missing_cols = required_columns - set(df.columns)
            error_msg = f"CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required_columns}. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {missing_cols}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        if QUESTION_NUMBER_COLUMN_NAME not in df.columns:
            error_msg = f"CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É: {QUESTION_NUMBER_COLUMN_NAME}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        predictor = ModelPredictor(model_path)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        logger.info("–ù–∞—á–∞–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ü–µ–Ω–æ–∫...")
        scores = []
        confidences = []
        processed_count = 0

        for idx, row in df.iterrows():
            question_num = row[QUESTION_NUMBER_COLUMN_NAME]
            question_text = row.get('–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', '')
            answer_text = row.get('–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', '')

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
            training_text = predictor.create_training_text(question_text, answer_text, question_num)

            if not training_text or training_text == "–í–æ–ø—Ä–æ—Å: \n\n–û—Ç–≤–µ—Ç: ":
                logger.warning(f"–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {idx}, –≤–æ–ø—Ä–æ—Å {question_num}")
                score = 0
                confidence = 0.0
            else:
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                predicted_class, confidence = predictor.predict_single_text(training_text)
                score = predictor.map_class_to_score(predicted_class, question_num)
                processed_count += 1

            scores.append(score)
            confidences.append(confidence)


            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if (idx + 1) % 100 == 0 or (idx + 1) == len(df):
                progress_percent = int(((idx + 1) / len(df)) * 100)  # TODO –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1}/{len(df)} –∑–∞–ø–∏—Å–µ–π ({progress_percent}%)")


        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        df[SCORE_COLUMN_NAME] = scores
        df['–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è'] = confidences

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        df.to_csv(output_file_path, index=False, encoding='utf-8', sep=';')
        logger.info(
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –∑–∞–ø–∏—Å–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file_path}")

        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
        if output_file_path.exists():
            output_file_path.unlink(missing_ok=True)
            logger.info("–£–¥–∞–ª–µ–Ω —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")
        raise e


def process_csv_placeholder(input_file_path: Path, output_file_path: Path):
    """
    –†–µ–∑–µ—Ä–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV-—Ñ–∞–π–ª–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.
    """
    logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è placeholder-—Ä–µ–∂–∏–º (—Å–ª—É—á–∞–π–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏)")

    try:
        try:
            df = pd.read_csv(input_file_path, sep=';', encoding=CSV_ENCODING)
        except pd.errors.ParserError:
            df = pd.read_csv(input_file_path, sep=',', encoding=CSV_ENCODING)

        required_columns = set(CSV_COLUMN_CONFIG["INPUT_COLUMNS"])
        if not all(col in df.columns for col in required_columns):
            missing_cols = required_columns - set(df.columns)
            error_msg = f"CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required_columns}. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {missing_cols}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        if QUESTION_NUMBER_COLUMN_NAME not in df.columns:
            error_msg = f"CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É: {QUESTION_NUMBER_COLUMN_NAME}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        score_ranges = {
            1: (0, 1),
            2: (0, 2),
            3: (0, 1),
            4: (0, 2)
        }

        scores = []
        for _, row in df.iterrows():
            q_num = row[QUESTION_NUMBER_COLUMN_NAME]
            if q_num in score_ranges:
                min_score, max_score = score_ranges[q_num]
                score = np.random.randint(min_score, max_score + 1)
            else:
                score = 0
            scores.append(score)

        df[SCORE_COLUMN_NAME] = scores
        df.to_csv(output_file_path, index=False, encoding='utf-8', sep=';')
        logger.info(f"Placeholder –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file_path}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ placeholder-–æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        if output_file_path.exists():
            output_file_path.unlink(missing_ok=True)
        raise e


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–∑ –±—ç–∫–µ–Ω–¥–∞
def process_exam_csv(input_file_path: Path, output_file_path: Path, use_model: bool = True):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–∫–∑–∞–º–µ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö CSV-—Ñ–∞–π–ª–æ–≤.

    Args:
        input_file_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV-—Ñ–∞–π–ª—É
        output_file_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        use_model: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (True) –∏–ª–∏ –∑–∞–≥–ª—É—à–∫—É (False)
    """
    try:
        if use_model:
            return process_csv_with_model(input_file_path, output_file_path)
        else:
            return process_csv_placeholder(input_file_path, output_file_path)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ CSV: {e}")
        raise
